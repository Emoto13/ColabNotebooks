{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"summarizer.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1P1vi4p4uje5OlCp9bdNqlbdxqUCX-qPJ","authorship_tag":"ABX9TyN7AJp0I1extht6swPTmZxm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ImPCIsrTE5sF","colab_type":"code","outputId":"08d02672-f289-4d78-c8c6-28bb35ca440f","executionInfo":{"status":"ok","timestamp":1588996034233,"user_tz":-330,"elapsed":1916,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd /content/drive/My Drive/Colab Notebooks/summarizer_transformer/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/summarizer_transformer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zbxhyl_zFlWL","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import time\n","import re\n","import pickle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yH5cg5pSIHaZ","colab_type":"text"},"source":["### Loading Data"]},{"cell_type":"code","metadata":{"id":"K_AjGkWXITKA","colab_type":"code","colab":{}},"source":["news = pd.read_excel(\"data/news.xlsx\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-rYZhayIe9x","colab_type":"code","colab":{}},"source":["news.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXtxc-toIc94","colab_type":"code","outputId":"d39eac33-f967-492d-e6f7-a2331bb638c0","executionInfo":{"status":"ok","timestamp":1588996050470,"user_tz":-330,"elapsed":15455,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}},"colab":{"base_uri":"https://localhost:8080/","height":206}},"source":["news.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Headline</th>\n","      <th>Short</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4 ex-bank officials booked for cheating bank o...</td>\n","      <td>The CBI on Saturday booked four former officia...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Supreme Court to go paperless in 6 months: CJI</td>\n","      <td>Chief Justice JS Khehar has said the Supreme C...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>At least 3 killed, 30 injured in blast in Sylh...</td>\n","      <td>At least three people were killed, including a...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Why has Reliance been barred from trading in f...</td>\n","      <td>Mukesh Ambani-led Reliance Industries (RIL) wa...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Was stopped from entering my own studio at Tim...</td>\n","      <td>TV news anchor Arnab Goswami has said he was t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Headline                                              Short\n","0  4 ex-bank officials booked for cheating bank o...  The CBI on Saturday booked four former officia...\n","1     Supreme Court to go paperless in 6 months: CJI  Chief Justice JS Khehar has said the Supreme C...\n","2  At least 3 killed, 30 injured in blast in Sylh...  At least three people were killed, including a...\n","3  Why has Reliance been barred from trading in f...  Mukesh Ambani-led Reliance Industries (RIL) wa...\n","4  Was stopped from entering my own studio at Tim...  TV news anchor Arnab Goswami has said he was t..."]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"vR2hg9themaN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"53a9c60b-3eb2-4f53-9ee5-a427a9f1f1d7","executionInfo":{"status":"ok","timestamp":1588996050471,"user_tz":-330,"elapsed":14794,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["news.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(55104, 2)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"d4cEp3wmI2BX","colab_type":"code","colab":{}},"source":["document = news['Short']\n","summary = news['Headline']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2z55AhpKIdK7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"ed19e222-2f75-4a6b-d83b-6f021544d9aa","executionInfo":{"status":"ok","timestamp":1588996050474,"user_tz":-330,"elapsed":13372,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["document[30], summary[30]"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('According to the Guinness World Records, the most generations alive in a single family have been seven.  The difference between the oldest and the youngest person in the family was about 109 years, when Augusta Bunge&#39;s great-great-great-great grandson was born on January 21, 1989. The family belonged to the United States of America.',\n"," 'The most generations alive in a single family have been 7')"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"f8gKyq1gIq4r","colab_type":"text"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"TJ6LE4MrJjC_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"cb4f9812-a59f-4bfc-9111-e95c6c72c5f6","executionInfo":{"status":"ok","timestamp":1588996050475,"user_tz":-330,"elapsed":12201,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["# for decoder sequence\n","summary = summary.apply(lambda x: '<go> ' + x + ' <stop>')\n","summary.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    <go> 4 ex-bank officials booked for cheating b...\n","1    <go> Supreme Court to go paperless in 6 months...\n","2    <go> At least 3 killed, 30 injured in blast in...\n","3    <go> Why has Reliance been barred from trading...\n","4    <go> Was stopped from entering my own studio a...\n","Name: Headline, dtype: object"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"95Zv7FIvKbTi","colab_type":"text"},"source":["#### Tokenizing the texts into integer tokens"]},{"cell_type":"code","metadata":{"id":"7TqbpEyPMRqa","colab_type":"code","colab":{}},"source":["# since < and > from default tokens cannot be removed\n","filters = '!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n'\n","oov_token = '<unk>'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHw2csoYImsa","colab_type":"code","colab":{}},"source":["document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n","summary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWU9Xu7OKVab","colab_type":"code","colab":{}},"source":["document_tokenizer.fit_on_texts(document)\n","summary_tokenizer.fit_on_texts(summary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ESm-aYR-tvx","colab_type":"code","colab":{}},"source":["inputs = document_tokenizer.texts_to_sequences(document)\n","targets = summary_tokenizer.texts_to_sequences(summary)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVyErXAei5_b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"41027c54-ce96-46d2-85c1-ff09bfe5d7dd","executionInfo":{"status":"ok","timestamp":1588996264681,"user_tz":-330,"elapsed":1240,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["summary_tokenizer.texts_to_sequences([\"This is a test\"])"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[184, 22, 12, 71]]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"Ryx9qx90jwXu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c4b8270a-e497-47a5-e93b-90a328a7f0a9","executionInfo":{"status":"ok","timestamp":1588996308592,"user_tz":-330,"elapsed":1268,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["summary_tokenizer.sequences_to_texts([[184, 22, 12, 71]])"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this is a test']"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"KoizyBvLKv8h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"62198745-1a53-41ef-f10c-5c9ca315fbe4","executionInfo":{"status":"ok","timestamp":1588996326633,"user_tz":-330,"elapsed":1311,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["encoder_vocab_size = len(document_tokenizer.word_index) + 1\n","decoder_vocab_size = len(summary_tokenizer.word_index) + 1\n","\n","# vocab_size\n","encoder_vocab_size, decoder_vocab_size"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(76362, 29661)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"mZden_q9_eZr","colab_type":"text"},"source":["#### Obtaining insights on lengths for defining maxlen"]},{"cell_type":"code","metadata":{"id":"ma4o2nGdK5Xb","colab_type":"code","colab":{}},"source":["document_lengths = pd.Series([len(x) for x in document])\n","summary_lengths = pd.Series([len(x) for x in summary])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iXZlO99C-UXK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"8b1480e7-5481-43f0-b685-e1deecda4548","executionInfo":{"status":"ok","timestamp":1588996333925,"user_tz":-330,"elapsed":1365,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["document_lengths.describe()"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    55104.000000\n","mean       368.003049\n","std         26.235510\n","min        280.000000\n","25%        350.000000\n","50%        369.000000\n","75%        387.000000\n","max        469.000000\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"ALMwKMx--ZF7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"df49e521-f058-466e-ec7c-f667c215c748","executionInfo":{"status":"ok","timestamp":1588996333927,"user_tz":-330,"elapsed":1139,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["summary_lengths.describe()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["count    55104.000000\n","mean        63.620282\n","std          7.267463\n","min         20.000000\n","25%         59.000000\n","50%         63.000000\n","75%         69.000000\n","max         96.000000\n","dtype: float64"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"cVeMilXr-bpC","colab_type":"code","colab":{}},"source":["# maxlen\n","# taking values > and round figured to 75th percentile\n","# at the same time not leaving high variance\n","encoder_maxlen = 400\n","decoder_maxlen = 75"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SWap3YJBk-D","colab_type":"text"},"source":["#### Padding/Truncating sequences for identical sequence lengths"]},{"cell_type":"code","metadata":{"id":"vEyUBeu7ACRt","colab_type":"code","colab":{}},"source":["inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=encoder_maxlen, padding='post', truncating='post')\n","targets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=decoder_maxlen, padding='post', truncating='post')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wIP0kIIcB8Rm","colab_type":"text"},"source":["### Creating dataset pipeline"]},{"cell_type":"code","metadata":{"id":"LzO6l3-AB7hJ","colab_type":"code","colab":{}},"source":["inputs = tf.cast(inputs, dtype=tf.int32)\n","targets = tf.cast(targets, dtype=tf.int32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"slZ5f4P4DurS","colab_type":"code","colab":{}},"source":["BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wI-fV7eABWN6","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isN1CpAXLfsl","colab_type":"text"},"source":["### Positional Encoding for adding notion of position among words as unlike RNN this is non-directional"]},{"cell_type":"code","metadata":{"id":"Purv7oyhETDZ","colab_type":"code","colab":{}},"source":["def get_angles(position, i, d_model):\n","    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","    return position * angle_rates"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"40J2pc2NEXp5","colab_type":"code","colab":{}},"source":["def positional_encoding(position, d_model):\n","    angle_rads = get_angles(\n","        np.arange(position)[:, np.newaxis],\n","        np.arange(d_model)[np.newaxis, :],\n","        d_model\n","    )\n","\n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","    pos_encoding = angle_rads[np.newaxis, ...]\n","\n","    return tf.cast(pos_encoding, dtype=tf.float32)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24Pe01DMMWHc","colab_type":"text"},"source":["### Masking\n","\n","- Padding mask for masking \"pad\" sequences\n","- Lookahead mask for masking future words from contributing in prediction of current words in self attention"]},{"cell_type":"code","metadata":{"id":"hN1wVQAdMVYy","colab_type":"code","colab":{}},"source":["def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    return seq[:, tf.newaxis, tf.newaxis, :]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmjAPLWuMREE","colab_type":"code","colab":{}},"source":["def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8DqUBc4NFOy","colab_type":"text"},"source":["### Building the Model"]},{"cell_type":"markdown","metadata":{"id":"WfknVF7hNKf7","colab_type":"text"},"source":["#### Scaled Dot Product"]},{"cell_type":"code","metadata":{"id":"w_B6M9OBNBKB","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","    output = tf.matmul(attention_weights, v)\n","    return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rf7_a5uQOfJk","colab_type":"text"},"source":["#### Multi-Headed Attention"]},{"cell_type":"code","metadata":{"id":"iIuFrdXnNZEC","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)\n","        k = self.wk(k)\n","        v = self.wv(v)\n","\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","\n","        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","        output = self.dense(concat_attention)\n","            \n","        return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A49tXMVvOkOZ","colab_type":"text"},"source":["### Feed Forward Network"]},{"cell_type":"code","metadata":{"id":"d9-qoKuTNwKq","colab_type":"code","colab":{}},"source":["def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),\n","        tf.keras.layers.Dense(d_model)\n","    ])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B2RRmn2bOpW9","colab_type":"text"},"source":["#### Fundamental Unit of Transformer encoder"]},{"cell_type":"code","metadata":{"id":"HNuoJoFWO335","colab_type":"code","colab":{}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, training, mask):\n","        attn_output, _ = self.mha(x, x, x, mask)\n","        attn_output = self.dropout1(attn_output, training=training)\n","        out1 = self.layernorm1(x + attn_output)\n","\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output, training=training)\n","        out2 = self.layernorm2(out1 + ffn_output)\n","\n","        return out2\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9i6Zh8gnPqdW","colab_type":"text"},"source":["#### Fundamental Unit of Transformer decoder"]},{"cell_type":"code","metadata":{"id":"7CVmvs6dPMRC","colab_type":"code","colab":{}},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1):\n","        super(DecoderLayer, self).__init__()\n","\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","        self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(attn1 + x)\n","\n","        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","        attn2 = self.dropout2(attn2, training=training)\n","        out2 = self.layernorm2(attn2 + out1)\n","\n","        ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        out3 = self.layernorm3(ffn_output + out2)\n","\n","        return out3, attn_weights_block1, attn_weights_block2\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6zt5MUc_QNid","colab_type":"text"},"source":["#### Encoder consisting of multiple EncoderLayer(s)"]},{"cell_type":"code","metadata":{"id":"BrbnTwijQJ-h","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Encoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","    def call(self, x, training, mask):\n","        seq_len = tf.shape(x)[1]\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","    \n","        for i in range(self.num_layers):\n","            x = self.enc_layers[i](x, training, mask)\n","    \n","        return x\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4N5LrNrvRexg","colab_type":"text"},"source":["#### Decoder consisting of multiple DecoderLayer(s)"]},{"cell_type":"code","metadata":{"id":"UmeqkZrIRbSB","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","        super(Decoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","\n","        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","        seq_len = tf.shape(x)[1]\n","        attention_weights = {}\n","\n","        x = self.embedding(x)\n","        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","        x += self.pos_encoding[:, :seq_len, :]\n","\n","        x = self.dropout(x, training=training)\n","\n","        for i in range(self.num_layers):\n","            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","\n","            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","        return x, attention_weights\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbMNK_bzSHnh","colab_type":"text"},"source":["#### Finally, the Transformer"]},{"cell_type":"code","metadata":{"id":"FXHRG-o4R9Mc","colab_type":"code","colab":{}},"source":["class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","        super(Transformer, self).__init__()\n","\n","        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","\n","        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","\n","        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","        enc_output = self.encoder(inp, training, enc_padding_mask)\n","\n","        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","        final_output = self.final_layer(dec_output)\n","\n","        return final_output, attention_weights\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UndsMPZXTdSr","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"lMTZJdIoSbuy","colab_type":"code","colab":{}},"source":["# hyper-params\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","EPOCHS = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uOGvkYDNTjIj","colab_type":"text"},"source":["#### Adam optimizer with custom learning rate scheduling"]},{"cell_type":"code","metadata":{"id":"tfiynCLlTL8C","colab_type":"code","colab":{}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, d_model, warmup_steps=4000):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.d_model = d_model\n","        self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","        self.warmup_steps = warmup_steps\n","    \n","    def __call__(self, step):\n","        arg1 = tf.math.rsqrt(step)\n","        arg2 = step * (self.warmup_steps ** -1.5)\n","\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DsVdrENTUERY","colab_type":"text"},"source":["#### Defining losses and other metrics "]},{"cell_type":"code","metadata":{"id":"Ip1-943kTXXK","colab_type":"code","colab":{}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ktKwyvKtTvF6","colab_type":"code","colab":{}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uW4LA_45T4Aa","colab_type":"code","colab":{}},"source":["def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","\n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ze0u6xxXT7dI","colab_type":"code","colab":{}},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9XvKy3v6ULnO","colab_type":"text"},"source":["#### Transformer"]},{"cell_type":"code","metadata":{"id":"d5-RcxqFUCuk","colab_type":"code","colab":{}},"source":["transformer = Transformer(\n","    num_layers, \n","    d_model, \n","    num_heads, \n","    dff,\n","    encoder_vocab_size, \n","    decoder_vocab_size, \n","    pe_input=encoder_vocab_size, \n","    pe_target=decoder_vocab_size,\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f56BGiVXU_Dk","colab_type":"text"},"source":["#### Masks"]},{"cell_type":"code","metadata":{"id":"FZxHuyZxU5Pa","colab_type":"code","colab":{}},"source":["def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)\n","    dec_padding_mask = create_padding_mask(inp)\n","\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","    return enc_padding_mask, combined_mask, dec_padding_mask\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SYIotvaBVI0d","colab_type":"text"},"source":["#### Checkpoints"]},{"cell_type":"code","metadata":{"id":"tOc1_3c-VGaL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"eeab15d7-f887-4f37-dfec-c980948f97d6","executionInfo":{"status":"ok","timestamp":1588996357748,"user_tz":-330,"elapsed":7786,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["checkpoint_path = \"checkpoints\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')"],"execution_count":53,"outputs":[{"output_type":"stream","text":["Latest checkpoint restored!!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WfpI0gS4c06c","colab_type":"text"},"source":["#### Training steps"]},{"cell_type":"code","metadata":{"id":"xmVOMzkrczgl","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","\n","    with tf.GradientTape() as tape:\n","        predictions, _ = transformer(\n","            inp, tar_inp, \n","            True, \n","            enc_padding_mask, \n","            combined_mask, \n","            dec_padding_mask\n","        )\n","        loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","    train_loss(loss)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xORKpv69dSW5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"572d3232-3af4-4bd6-ebc7-a151f6ad000d","executionInfo":{"status":"ok","timestamp":1589005387811,"user_tz":-330,"elapsed":6000532,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    train_loss.reset_states()\n","  \n","    for (batch, (inp, tar)) in enumerate(dataset):\n","        train_step(inp, tar)\n","    \n","        # 55k samples\n","        # we display 3 batch results -- 0th, middle and last one (approx)\n","        # 55k / 64 ~ 858; 858 / 2 = 429\n","        if batch % 429 == 0:\n","            print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n","      \n","    if (epoch + 1) % 5 == 0:\n","        ckpt_save_path = ckpt_manager.save()\n","        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n","    \n","    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n","\n","    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"],"execution_count":86,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 2.4681\n","Epoch 1 Batch 429 Loss 2.4650\n","Epoch 1 Batch 858 Loss 2.5071\n","Epoch 1 Loss 2.5077\n","Time taken for 1 epoch: 308.9519073963165 secs\n","\n","Epoch 2 Batch 0 Loss 2.3482\n","Epoch 2 Batch 429 Loss 2.4071\n","Epoch 2 Batch 858 Loss 2.4461\n","Epoch 2 Loss 2.4464\n","Time taken for 1 epoch: 299.0744743347168 secs\n","\n","Epoch 3 Batch 0 Loss 2.3197\n","Epoch 3 Batch 429 Loss 2.3417\n","Epoch 3 Batch 858 Loss 2.3879\n","Epoch 3 Loss 2.3883\n","Time taken for 1 epoch: 301.2433364391327 secs\n","\n","Epoch 4 Batch 0 Loss 2.1710\n","Epoch 4 Batch 429 Loss 2.2931\n","Epoch 4 Batch 858 Loss 2.3333\n","Epoch 4 Loss 2.3340\n","Time taken for 1 epoch: 301.5991175174713 secs\n","\n","Epoch 5 Batch 0 Loss 2.1340\n","Epoch 5 Batch 429 Loss 2.2443\n","Epoch 5 Batch 858 Loss 2.2869\n","Saving checkpoint for epoch 5 at checkpoints/ckpt-5\n","Epoch 5 Loss 2.2872\n","Time taken for 1 epoch: 302.6865613460541 secs\n","\n","Epoch 6 Batch 0 Loss 2.2356\n","Epoch 6 Batch 429 Loss 2.1970\n","Epoch 6 Batch 858 Loss 2.2388\n","Epoch 6 Loss 2.2395\n","Time taken for 1 epoch: 299.43392634391785 secs\n","\n","Epoch 7 Batch 0 Loss 2.0859\n","Epoch 7 Batch 429 Loss 2.1447\n","Epoch 7 Batch 858 Loss 2.1965\n","Epoch 7 Loss 2.1969\n","Time taken for 1 epoch: 299.074423789978 secs\n","\n","Epoch 8 Batch 0 Loss 2.0642\n","Epoch 8 Batch 429 Loss 2.1070\n","Epoch 8 Batch 858 Loss 2.1567\n","Epoch 8 Loss 2.1568\n","Time taken for 1 epoch: 299.17002749443054 secs\n","\n","Epoch 9 Batch 0 Loss 2.0930\n","Epoch 9 Batch 429 Loss 2.0631\n","Epoch 9 Batch 858 Loss 2.1183\n","Epoch 9 Loss 2.1191\n","Time taken for 1 epoch: 299.0372166633606 secs\n","\n","Epoch 10 Batch 0 Loss 1.9020\n","Epoch 10 Batch 429 Loss 2.0271\n","Epoch 10 Batch 858 Loss 2.0765\n","Saving checkpoint for epoch 10 at checkpoints/ckpt-6\n","Epoch 10 Loss 2.0768\n","Time taken for 1 epoch: 299.7342073917389 secs\n","\n","Epoch 11 Batch 0 Loss 1.9392\n","Epoch 11 Batch 429 Loss 1.9937\n","Epoch 11 Batch 858 Loss 2.0476\n","Epoch 11 Loss 2.0478\n","Time taken for 1 epoch: 299.45343947410583 secs\n","\n","Epoch 12 Batch 0 Loss 1.7920\n","Epoch 12 Batch 429 Loss 1.9643\n","Epoch 12 Batch 858 Loss 2.0171\n","Epoch 12 Loss 2.0174\n","Time taken for 1 epoch: 298.78548765182495 secs\n","\n","Epoch 13 Batch 0 Loss 1.7588\n","Epoch 13 Batch 429 Loss 1.9253\n","Epoch 13 Batch 858 Loss 1.9814\n","Epoch 13 Loss 1.9816\n","Time taken for 1 epoch: 298.87867879867554 secs\n","\n","Epoch 14 Batch 0 Loss 1.8428\n","Epoch 14 Batch 429 Loss 1.8962\n","Epoch 14 Batch 858 Loss 1.9524\n","Epoch 14 Loss 1.9527\n","Time taken for 1 epoch: 298.65805077552795 secs\n","\n","Epoch 15 Batch 0 Loss 1.6725\n","Epoch 15 Batch 429 Loss 1.8697\n","Epoch 15 Batch 858 Loss 1.9249\n","Saving checkpoint for epoch 15 at checkpoints/ckpt-7\n","Epoch 15 Loss 1.9251\n","Time taken for 1 epoch: 299.3562548160553 secs\n","\n","Epoch 16 Batch 0 Loss 1.8478\n","Epoch 16 Batch 429 Loss 1.8391\n","Epoch 16 Batch 858 Loss 1.8983\n","Epoch 16 Loss 1.8990\n","Time taken for 1 epoch: 298.57386469841003 secs\n","\n","Epoch 17 Batch 0 Loss 1.6218\n","Epoch 17 Batch 429 Loss 1.8088\n","Epoch 17 Batch 858 Loss 1.8721\n","Epoch 17 Loss 1.8724\n","Time taken for 1 epoch: 298.404292345047 secs\n","\n","Epoch 18 Batch 0 Loss 1.8277\n","Epoch 18 Batch 429 Loss 1.7935\n","Epoch 18 Batch 858 Loss 1.8485\n","Epoch 18 Loss 1.8489\n","Time taken for 1 epoch: 298.76677560806274 secs\n","\n","Epoch 19 Batch 0 Loss 1.6240\n","Epoch 19 Batch 429 Loss 1.7632\n","Epoch 19 Batch 858 Loss 1.8245\n","Epoch 19 Loss 1.8252\n","Time taken for 1 epoch: 298.52965450286865 secs\n","\n","Epoch 20 Batch 0 Loss 1.7538\n","Epoch 20 Batch 429 Loss 1.7412\n","Epoch 20 Batch 858 Loss 1.8018\n","Saving checkpoint for epoch 20 at checkpoints/ckpt-8\n","Epoch 20 Loss 1.8020\n","Time taken for 1 epoch: 299.4963185787201 secs\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PVbEUCZagJ0G","colab_type":"text"},"source":["### Inference"]},{"cell_type":"markdown","metadata":{"id":"YMbqGTixu1cl","colab_type":"text"},"source":["#### Predicting one word at a time at the decoder and appending it to the output; then taking the complete sequence as an input to the decoder and repeating until maxlen or stop keyword appears"]},{"cell_type":"code","metadata":{"id":"F5D5cv2Jd8-6","colab_type":"code","colab":{}},"source":["def evaluate(input_document):\n","    input_document = document_tokenizer.texts_to_sequences([input_document])\n","    input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n","\n","    encoder_input = tf.expand_dims(input_document[0], 0)\n","\n","    decoder_input = [summary_tokenizer.word_index[\"<go>\"]]\n","    output = tf.expand_dims(decoder_input, 0)\n","    \n","    for i in range(decoder_maxlen):\n","        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","        predictions, attention_weights = transformer(\n","            encoder_input, \n","            output,\n","            False,\n","            enc_padding_mask,\n","            combined_mask,\n","            dec_padding_mask\n","        )\n","\n","        predictions = predictions[: ,-1:, :]\n","        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","\n","        if predicted_id == summary_tokenizer.word_index[\"<stop>\"]:\n","            return tf.squeeze(output, axis=0), attention_weights\n","\n","        output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkpdiW6wnmiS","colab_type":"code","colab":{}},"source":["def summarize(input_document):\n","    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n","    summarized = evaluate(input_document=input_document)[0].numpy()\n","    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n","    return summary_tokenizer.sequences_to_texts(summarized)[0]  # since there is just one translated document"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZoEHvIxrYKZ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"64212034-ffda-41f2-c830-fc1f17c674e6","executionInfo":{"status":"ok","timestamp":1589005779180,"user_tz":-330,"elapsed":1748,"user":{"displayName":"rohan jagtap","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFFnpJjw-7WaiTzz7xrkIJjBBwMs5i3OwVVYALIg=s64","userId":"07173842849534370372"}}},"source":["summarize(\n","    \"US-based private equity firm General Atlantic is in talks to invest about \\\n","    $850 million to $950 million in Reliance Industries' digital unit Jio \\\n","    Platforms, the Bloomberg reported. Saudi Arabia's $320 billion sovereign \\\n","    wealth fund is reportedly also exploring a potential investment in the \\\n","    Mukesh Ambani-led company. The 'Public Investment Fund' is looking to \\\n","    acquire a minority stake in Jio Platforms.\"\n",")"],"execution_count":94,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'reliance group buys stake in net profit for 250 bn'"]},"metadata":{"tags":[]},"execution_count":94}]},{"cell_type":"code","metadata":{"id":"kNVOWPXFIn0k","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}